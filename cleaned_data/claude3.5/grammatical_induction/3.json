[
    {
        "file": "3.csv",
        "row": 2,
        "prompt": "New Vocab:\ngati (to catch)\n\npika (bug)\n\nRules:\nmyu = conditional (“would”).\n\nmye = past marker.\n\nVerbs stack after i.\n\nObject marked with e.\n\nSentence Sample:\nbegu bay i myu mye gati e pika.\nQuestion:\nIs this sentence grammatical according to the rules above? Translate it to English.",
        "targets": [
            "If I had caught the bug.",
            "Yes, it is grammatical.\nTranslation: If I would have caught the bug."
        ],
        "actual": "Yes. The boy would have caught the bug.",
        "raw_target": "Yes, it is grammatical.\nTranslation: If I would have caught the bug. / If I had caught the bug.",
        "raw_actual": "Yes. \"The boy would have caught the bug.\"",
        "bleu_score": 55.55238068023583,
        "chrF_score": 57.75900780912898,
        "chrF_plus_score": 55.66070782921201,
        "ter_test": 55.55555555555556,
        "bert_score_p": 0.9172773361206055,
        "bert_score_r": 0.9157662391662598,
        "bert_score_f1": 0.9121605157852173
    },
    {
        "file": "3.csv",
        "row": 3,
        "prompt": "New Vocab:\nsene (clean)\n\nkwe (invert prefix)\n\nRules:\ngen = abstract noun suffix (“-ness”).\n\nkwe inverts root before gen.\n\ne marks predicate noun.\n\nSentence Sample:\nswe e kwe senegen.\nQuestion:\nWhat does kwe senegen mean?",
        "targets": [
            "Uncleanliness",
            "dirtiness"
        ],
        "actual": "dirty-ness",
        "raw_target": "Uncleanliness/ dirtiness",
        "raw_actual": "dirty-ness",
        "bleu_score": 0.0,
        "chrF_score": 41.325370110490674,
        "chrF_plus_score": 35.42174580899201,
        "ter_test": 100.0,
        "bert_score_p": 0.8596979379653931,
        "bert_score_r": 0.8822661638259888,
        "bert_score_f1": 0.8708358407020569
    },
    {
        "file": "3.csv",
        "row": 4,
        "prompt": "New Vocab:\nlato (liquid)\n\npodo (dirt)\n\nlaw (inside)\n\nRules:\nboy = preposition marker.\n\nlaw = inside.\n\nLocation phrase follows verb.\n\nboy can prefix location word.\n\nSentence Sample:\nswe i hay e lato boylaw podo.\nQuestion:\nIs this sentence grammatical? Where is the liquid?",
        "targets": [
            "Yes, it is grammatical.\nThe liquid is inside the dirt."
        ],
        "actual": "The liquid is inside the dirt.",
        "raw_target": "Target Output:\nYes, it is grammatical.\nThe liquid is inside the dirt.",
        "raw_actual": "Yes, the sentence is grammatical.\nThe liquid is inside the dirt.",
        "bleu_score": 42.43728456769501,
        "chrF_score": 58.367258943490384,
        "chrF_plus_score": 58.137505474207614,
        "ter_test": 40.0,
        "bert_score_p": 0.9453890323638916,
        "bert_score_r": 0.916304886341095,
        "bert_score_f1": 0.9306197762489319
    },
    {
        "file": "3.csv",
        "row": 5,
        "prompt": "New Vocab:\nsuki (to swim)\n\nliso (water)\n\nRules:\nCommands: omit subject, use i yoy.\n\nObject marked with e.\n\nVerb can be intransitive.\n\nSentence Sample:\ni yoy suki e liso!\nQuestion:\nWhat is this a command to do?",
        "targets": [
            "Swim in the water!"
        ],
        "actual": "swim the water!",
        "raw_target": "Swim in the water!",
        "raw_actual": "swim the water!",
        "bleu_score": 46.30777161991026,
        "chrF_score": 59.968223309751934,
        "chrF_plus_score": 59.3797177507787,
        "ter_test": 25.0,
        "bert_score_p": 0.948718249797821,
        "bert_score_r": 0.9504960775375366,
        "bert_score_f1": 0.9496063590049744
    },
    {
        "file": "3.csv",
        "row": 6,
        "prompt": "New Vocab:\nbese (pretty)\n\nhasu (flower)\n\nRules:\nbyu = most (superlative).\n\nQuestion starts with kay.\n\nAdjective follows e.\n\nSentence Sample:\nkay a pin hasu byubese e?\nQuestion:\nIs this sentence grammatical? explain.",
        "targets": [
            "No it is not grammatical."
        ],
        "actual": "No",
        "raw_target": "No it is not grammatical.",
        "raw_actual": "No",
        "bleu_score": 0.673794699908547,
        "chrF_score": 8.915521777258109,
        "chrF_plus_score": 12.666795590794818,
        "ter_test": 80.0,
        "bert_score_p": 0.8576772809028625,
        "bert_score_r": 0.835667610168457,
        "bert_score_f1": 0.8465294241905212
    }
]