[
    {
        "file": "1.csv",
        "row": 2,
        "prompt": "Word/Rule:\nmya = parent\nbey = related / -ly\n\nQuestion:\nWhat does myabey most likely mean?",
        "targets": [
            "parent-related",
            "parental"
        ],
        "actual": "parent-related",
        "raw_target": "parental / parent-related",
        "raw_actual": "parent-related",
        "bleu_score": 100.00000000000004,
        "chrF_score": 100.0,
        "chrF_plus_score": 100.0,
        "ter_test": 0.0,
        "bert_score_p": 1.0,
        "bert_score_r": 1.0,
        "bert_score_f1": 1.0
    },
    {
        "file": "1.csv",
        "row": 3,
        "prompt": "Word/Rule:\nbya = male\nmya = parent\n\nQuestion:\nWhat does byamya most likely mean?",
        "targets": [
            "father"
        ],
        "actual": "male parent",
        "raw_target": "father",
        "raw_actual": "male parent",
        "bleu_score": 0.0,
        "chrF_score": 9.803921568627452,
        "chrF_plus_score": 8.403361344537815,
        "ter_test": 200.0,
        "bert_score_p": 0.8572425246238708,
        "bert_score_r": 0.9067184925079346,
        "bert_score_f1": 0.88128662109375
    },
    {
        "file": "1.csv",
        "row": 4,
        "prompt": "Word/Rule:\ndomay = friend\nbay = I / me\n\nQuestion:\nWhat does bay domay most likely mean?",
        "targets": [
            "my friend"
        ],
        "actual": "I friend",
        "raw_target": "my friend",
        "raw_actual": "I friend",
        "bleu_score": 49.99999999999999,
        "chrF_score": 61.76733753831832,
        "chrF_plus_score": 52.610873637968915,
        "ter_test": 50.0,
        "bert_score_p": 0.9960837364196777,
        "bert_score_r": 0.9960837364196777,
        "bert_score_f1": 0.9960837364196777
    },
    {
        "file": "1.csv",
        "row": 5,
        "prompt": "Word/Rule:\nkyu = you\nmoy = multiple\n\nQuestion:\nWhat does moykyu most likely mean?",
        "targets": [
            "you",
            "you plural"
        ],
        "actual": "multiple you",
        "raw_target": "you (plural)",
        "raw_actual": "multiple you",
        "bleu_score": 49.99999999999999,
        "chrF_score": 54.71412343809163,
        "chrF_plus_score": 65.01454192922928,
        "ter_test": 66.66666666666666,
        "bert_score_p": 0.8600623607635498,
        "bert_score_r": 0.8887748718261719,
        "bert_score_f1": 0.8686894178390503
    },
    {
        "file": "1.csv",
        "row": 6,
        "prompt": "Word/Rule:\ndune = number\ngen = concept / -ness\n\nQuestion:\nWhat does dungen most likely mean?",
        "targets": [
            "amount-ness",
            "quantity"
        ],
        "actual": "numberness",
        "raw_target": "quantity / amount-ness",
        "raw_actual": "numberness",
        "bleu_score": 0.0,
        "chrF_score": 21.82964639712111,
        "chrF_plus_score": 18.71112548324667,
        "ter_test": 100.0,
        "bert_score_p": 0.9426332712173462,
        "bert_score_r": 0.9069703221321106,
        "bert_score_f1": 0.9175617098808289
    },
    {
        "file": "1.csv",
        "row": 7,
        "prompt": "lenu = art\nsyu = to do / make\n\nQuestion:\nWhat does lenusyu most likely mean?",
        "targets": [
            "to make art"
        ],
        "actual": "art to do / make",
        "raw_target": "to make art",
        "raw_actual": "art to do / make",
        "bleu_score": 14.058533129758727,
        "chrF_score": 36.365393473903744,
        "chrF_plus_score": 38.36063149124871,
        "ter_test": 100.0,
        "bert_score_p": 0.867417573928833,
        "bert_score_r": 0.8991166949272156,
        "bert_score_f1": 0.8829826712608337
    },
    {
        "file": "1.csv",
        "row": 8,
        "prompt": "bote = blue\nsun = similar / -like\n\nQuestion:\nWhat does botesun most likely mean?",
        "targets": [
            "blue-like",
            "bluish"
        ],
        "actual": "blue-like",
        "raw_target": "blue-like / bluish",
        "raw_actual": "blue-like",
        "bleu_score": 100.00000000000004,
        "chrF_score": 100.0,
        "chrF_plus_score": 100.0,
        "ter_test": 0.0,
        "bert_score_p": 1.0,
        "bert_score_r": 1.0,
        "bert_score_f1": 1.0
    },
    {
        "file": "1.csv",
        "row": 9,
        "prompt": "Word/Rule:\npila = bird\nlase = fast\n\nQuestion:\nWhat does lase pila most likely mean?",
        "targets": [
            "fast bird"
        ],
        "actual": "fast bird",
        "raw_target": "fast bird",
        "raw_actual": "fast bird",
        "bleu_score": 100.00000000000004,
        "chrF_score": 100.0,
        "chrF_plus_score": 100.0,
        "ter_test": 0.0,
        "bert_score_p": 1.0000001192092896,
        "bert_score_r": 1.0000001192092896,
        "bert_score_f1": 1.0000001192092896
    },
    {
        "file": "1.csv",
        "row": 10,
        "prompt": "Word/Rule:\nhagu = story\ngoy = thing / object\n\nQuestion:\nWhat does hagugoy most likely mean?\n\nTarget Output:\nstory-thing / book / tale object",
        "targets": [
            "Story-thing",
            "Storybook",
            "book"
        ],
        "actual": "story-thing / tale object",
        "raw_target": "Story-thing / Storybook / book",
        "raw_actual": "story-thing / tale object\n",
        "bleu_score": 0.0,
        "chrF_score": 69.51697183293743,
        "chrF_plus_score": 59.58597585680352,
        "ter_test": 300.0,
        "bert_score_p": 0.8735429644584656,
        "bert_score_r": 0.9399617910385132,
        "bert_score_f1": 0.9055361151695251
    },
    {
        "file": "1.csv",
        "row": 11,
        "prompt": "Word/Rule:\nswa = to be\nlase = fast\n\nQuestion:\nIn swe i swa lase, what does lase modify?",
        "targets": [
            "The verb “to be” — so “they are quickly”"
        ],
        "actual": "swa",
        "raw_target": "The verb “to be” — so “they are quickly”",
        "raw_actual": "swa",
        "bleu_score": 0.0,
        "chrF_score": 2.5445292620865136,
        "chrF_plus_score": 1.9083969465648856,
        "ter_test": 100.0,
        "bert_score_p": 0.8128330707550049,
        "bert_score_r": 0.7778220176696777,
        "bert_score_f1": 0.7949422597885132
    }
]